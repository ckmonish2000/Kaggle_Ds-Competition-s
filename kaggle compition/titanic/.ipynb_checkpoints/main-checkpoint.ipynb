{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>455.366120</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>1.191257</td>\n",
       "      <td>35.674426</td>\n",
       "      <td>0.464481</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>78.682469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>247.052476</td>\n",
       "      <td>0.470725</td>\n",
       "      <td>0.515187</td>\n",
       "      <td>15.643866</td>\n",
       "      <td>0.644159</td>\n",
       "      <td>0.754617</td>\n",
       "      <td>76.347843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>263.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>457.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>676.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>890.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   183.000000  183.000000  183.000000  183.000000  183.000000   \n",
       "mean    455.366120    0.672131    1.191257   35.674426    0.464481   \n",
       "std     247.052476    0.470725    0.515187   15.643866    0.644159   \n",
       "min       2.000000    0.000000    1.000000    0.920000    0.000000   \n",
       "25%     263.500000    0.000000    1.000000   24.000000    0.000000   \n",
       "50%     457.000000    1.000000    1.000000   36.000000    0.000000   \n",
       "75%     676.000000    1.000000    1.000000   47.500000    1.000000   \n",
       "max     890.000000    1.000000    3.000000   80.000000    3.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  183.000000  183.000000  \n",
       "mean     0.475410   78.682469  \n",
       "std      0.754617   76.347843  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000   29.700000  \n",
       "50%      0.000000   57.000000  \n",
       "75%      1.000000   90.000000  \n",
       "max      4.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex\",\"Cabin\"]\n",
    "label=[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinlen=len(ds[\"Cabin\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cabin=le.fit_transform(ds[\"Cabin\"].unique())\n",
    "unencoded_cabin=ds[\"Cabin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cabinlen):\n",
    "    ds.loc[ds[\"Cabin\"]==unencoded_cabin[i],\"Cabin\"]=encoded_cabin[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# female=0 male=1\n",
    "ds.loc[ds[\"Sex\"]==\"female\",\"Sex\"]=0\n",
    "ds.loc[ds[\"Sex\"]==\"male\",\"Sex\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>72</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>48</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>117</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>131</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>43</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "1             2         1       1   \n",
       "3             4         1       1   \n",
       "6             7         0       1   \n",
       "10           11         1       3   \n",
       "11           12         1       1   \n",
       "\n",
       "                                                 Name Sex   Age  SibSp  Parch  \\\n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...   0  38.0      1      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)   0  35.0      1      0   \n",
       "6                             McCarthy, Mr. Timothy J   1  54.0      0      0   \n",
       "10                    Sandstrom, Miss. Marguerite Rut   0   4.0      1      1   \n",
       "11                           Bonnell, Miss. Elizabeth   0  58.0      0      0   \n",
       "\n",
       "      Ticket     Fare Cabin Embarked  \n",
       "1   PC 17599  71.2833    72        C  \n",
       "3     113803  53.1000    48        S  \n",
       "6      17463  51.8625   117        S  \n",
       "10   PP 9549  16.7000   131        S  \n",
       "11    113783  26.5500    43        S  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=ds[features]\n",
    "y=ds[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Odin Revolution\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Odin Revolution\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Odin Revolution\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Odin Revolution\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Odin Revolution\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X[\"SibSp\"]=X[\"SibSp\"].astype(\"float32\")\n",
    "X[\"Pclass\"]=X[\"Pclass\"].astype(\"float32\")\n",
    "X[\"Parch\"]=X[\"Parch\"].astype(\"float32\")\n",
    "X[\"Sex\"]=X[\"Sex\"].astype(\"float32\")\n",
    "X[\"Cabin\"]=X[\"Cabin\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass    float32\n",
       "Age       float64\n",
       "SibSp     float32\n",
       "Parch     float32\n",
       "Fare      float64\n",
       "Sex       float32\n",
       "Cabin     float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.from_numpy(X_train).float()\n",
    "y_train=torch.from_numpy(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=TensorDataset(X_train,y_train)\n",
    "dl=DataLoader(ts,batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(7,2)\n",
    "    def forward(self,xb):\n",
    "        xb=xb.reshape(-1,7)\n",
    "        out=self.linear(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Mymodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "loss=nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2338, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1926, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1677, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1517, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1405, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1318, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1245, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1179, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1117, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1056, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0932, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0804, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0737, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0599, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0527, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0454, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0379, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0303, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0225, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0145, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0065, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9982, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9899, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9554, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9465, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9285, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9194, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9103, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8918, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8824, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8543, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8449, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8355, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8261, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8074, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7981, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7795, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7703, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7612, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7342, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7167, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7081, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6996, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6912, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6591, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6440, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6296, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6159, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6094, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6032, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5913, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5857, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5753, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5705, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5617, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5577, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5473, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5418, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5323, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5311, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5281, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5273, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5275, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5277, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5285, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5297, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5301, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5305, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5309, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5313, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5317, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5325, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5328, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5332, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5336, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5339, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5343, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5346, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5349, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5352, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5355, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5360, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5369, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5371, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5372, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    for data,label in dl:\n",
    "        prde=model(data)\n",
    "        l=label.squeeze()\n",
    "        ls=loss(prde,l)\n",
    "        ls.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    print(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data,label in dl:\n",
    "    d=data\n",
    "    l=label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j=torch.max(model(d),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(l==j)/len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=torch.from_numpy(X_test).float()\n",
    "y_test=torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl=TensorDataset(X_test,y_test)\n",
    "dl=DataLoader(tl,batch_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=53.28964614868164 acc=23\n",
      "loss=53.071510314941406 acc=23\n",
      "loss=53.067352294921875 acc=23\n",
      "loss=53.06318283081055 acc=23\n",
      "loss=53.059024810791016 acc=23\n",
      "loss=53.054874420166016 acc=23\n",
      "loss=53.050716400146484 acc=23\n",
      "loss=53.046566009521484 acc=23\n",
      "loss=53.042415618896484 acc=23\n",
      "loss=53.038272857666016 acc=23\n",
      "loss=53.03413772583008 acc=23\n",
      "loss=53.029998779296875 acc=23\n",
      "loss=53.025856018066406 acc=23\n",
      "loss=53.021732330322266 acc=23\n",
      "loss=53.01759338378906 acc=23\n",
      "loss=53.013484954833984 acc=23\n",
      "loss=53.009361267089844 acc=23\n",
      "loss=53.00526428222656 acc=23\n",
      "loss=53.00114822387695 acc=23\n",
      "loss=52.997066497802734 acc=23\n",
      "loss=52.99296188354492 acc=23\n",
      "loss=52.98891830444336 acc=23\n",
      "loss=52.98481369018555 acc=23\n",
      "loss=52.980804443359375 acc=23\n",
      "loss=52.97669982910156 acc=23\n",
      "loss=52.972740173339844 acc=23\n",
      "loss=52.96860122680664 acc=23\n",
      "loss=52.96467971801758 acc=23\n",
      "loss=52.960506439208984 acc=23\n",
      "loss=52.956626892089844 acc=23\n",
      "loss=52.952392578125 acc=23\n",
      "loss=52.948551177978516 acc=23\n",
      "loss=52.944244384765625 acc=23\n",
      "loss=52.94041442871094 acc=23\n",
      "loss=52.93605041503906 acc=23\n",
      "loss=52.932228088378906 acc=23\n",
      "loss=52.92782974243164 acc=23\n",
      "loss=52.92402267456055 acc=23\n",
      "loss=52.919586181640625 acc=23\n",
      "loss=52.915794372558594 acc=23\n",
      "loss=52.91132736206055 acc=23\n",
      "loss=52.907535552978516 acc=23\n",
      "loss=52.90306091308594 acc=23\n",
      "loss=52.89926528930664 acc=23\n",
      "loss=52.8947868347168 acc=23\n",
      "loss=52.89100646972656 acc=23\n",
      "loss=52.88652420043945 acc=23\n",
      "loss=52.882720947265625 acc=23\n",
      "loss=52.87824630737305 acc=23\n",
      "loss=52.874446868896484 acc=23\n",
      "loss=52.86996841430664 acc=23\n",
      "loss=52.866172790527344 acc=23\n",
      "loss=52.8616943359375 acc=23\n",
      "loss=52.8578987121582 acc=23\n",
      "loss=52.85342025756836 acc=23\n",
      "loss=52.849632263183594 acc=23\n",
      "loss=52.84513473510742 acc=23\n",
      "loss=52.841339111328125 acc=23\n",
      "loss=52.83686065673828 acc=23\n",
      "loss=52.833065032958984 acc=23\n",
      "loss=52.828582763671875 acc=23\n",
      "loss=52.824790954589844 acc=23\n",
      "loss=52.820308685302734 acc=23\n",
      "loss=52.816505432128906 acc=23\n",
      "loss=52.812049865722656 acc=23\n",
      "loss=52.80824279785156 acc=23\n",
      "loss=52.80376052856445 acc=23\n",
      "loss=52.79998016357422 acc=23\n",
      "loss=52.79550552368164 acc=23\n",
      "loss=52.791709899902344 acc=23\n",
      "loss=52.7872314453125 acc=23\n",
      "loss=52.783451080322266 acc=23\n",
      "loss=52.77898025512695 acc=23\n",
      "loss=52.775211334228516 acc=23\n",
      "loss=52.77073669433594 acc=23\n",
      "loss=52.766971588134766 acc=23\n",
      "loss=52.76251983642578 acc=23\n",
      "loss=52.758766174316406 acc=23\n",
      "loss=52.75432586669922 acc=23\n",
      "loss=52.7506103515625 acc=23\n",
      "loss=52.74618911743164 acc=23\n",
      "loss=52.742515563964844 acc=23\n",
      "loss=52.738136291503906 acc=23\n",
      "loss=52.73455047607422 acc=23\n",
      "loss=52.7302360534668 acc=23\n",
      "loss=52.72679138183594 acc=23\n",
      "loss=52.722599029541016 acc=23\n",
      "loss=52.71938705444336 acc=23\n",
      "loss=52.71535873413086 acc=23\n",
      "loss=52.712547302246094 acc=23\n",
      "loss=52.70872497558594 acc=23\n",
      "loss=52.706485748291016 acc=23\n",
      "loss=52.702789306640625 acc=23\n",
      "loss=52.70124435424805 acc=23\n",
      "loss=52.69753646850586 acc=23\n",
      "loss=52.696678161621094 acc=23\n",
      "loss=52.692710876464844 acc=23\n",
      "loss=52.69235610961914 acc=23\n",
      "loss=52.68816375732422 acc=23\n",
      "loss=52.68808364868164 acc=23\n",
      "loss=52.68381118774414 acc=23\n",
      "loss=52.68385314941406 acc=23\n",
      "loss=52.679603576660156 acc=23\n",
      "loss=52.6796875 acc=23\n",
      "loss=52.675514221191406 acc=23\n",
      "loss=52.67560577392578 acc=23\n",
      "loss=52.671512603759766 acc=23\n",
      "loss=52.671600341796875 acc=23\n",
      "loss=52.66757583618164 acc=23\n",
      "loss=52.66764450073242 acc=23\n",
      "loss=52.6637077331543 acc=23\n",
      "loss=52.66374969482422 acc=23\n",
      "loss=52.65986251831055 acc=23\n",
      "loss=52.659889221191406 acc=23\n",
      "loss=52.656070709228516 acc=23\n",
      "loss=52.65608596801758 acc=23\n",
      "loss=52.65229415893555 acc=23\n",
      "loss=52.65230178833008 acc=23\n",
      "loss=52.648555755615234 acc=23\n",
      "loss=52.64854431152344 acc=23\n",
      "loss=52.64482498168945 acc=23\n",
      "loss=52.644805908203125 acc=23\n",
      "loss=52.6411247253418 acc=23\n",
      "loss=52.64108657836914 acc=23\n",
      "loss=52.637428283691406 acc=23\n",
      "loss=52.637386322021484 acc=23\n",
      "loss=52.63373947143555 acc=23\n",
      "loss=52.633697509765625 acc=23\n",
      "loss=52.630069732666016 acc=23\n",
      "loss=52.63002014160156 acc=23\n",
      "loss=52.626399993896484 acc=23\n",
      "loss=52.626346588134766 acc=23\n",
      "loss=52.62273406982422 acc=23\n",
      "loss=52.6226806640625 acc=23\n",
      "loss=52.619083404541016 acc=23\n",
      "loss=52.6190299987793 acc=23\n",
      "loss=52.61542510986328 acc=23\n",
      "loss=52.615379333496094 acc=23\n",
      "loss=52.611785888671875 acc=23\n",
      "loss=52.61172103881836 acc=23\n",
      "loss=52.6081428527832 acc=23\n",
      "loss=52.608089447021484 acc=23\n",
      "loss=52.6045036315918 acc=23\n",
      "loss=52.604461669921875 acc=23\n",
      "loss=52.60087203979492 acc=23\n",
      "loss=52.600833892822266 acc=23\n",
      "loss=52.59724807739258 acc=23\n",
      "loss=52.59723663330078 acc=23\n",
      "loss=52.593631744384766 acc=23\n",
      "loss=52.59365463256836 acc=23\n",
      "loss=52.59003829956055 acc=23\n",
      "loss=52.59010314941406 acc=23\n",
      "loss=52.58646774291992 acc=23\n",
      "loss=52.586612701416016 acc=23\n",
      "loss=52.58293914794922 acc=23\n",
      "loss=52.583187103271484 acc=23\n",
      "loss=52.579463958740234 acc=23\n",
      "loss=52.57990264892578 acc=23\n",
      "loss=52.57606506347656 acc=23\n",
      "loss=52.5767822265625 acc=23\n",
      "loss=52.572792053222656 acc=23\n",
      "loss=52.57390213012695 acc=23\n",
      "loss=52.56964874267578 acc=23\n",
      "loss=52.571346282958984 acc=23\n",
      "loss=52.56669998168945 acc=23\n",
      "loss=52.569122314453125 acc=23\n",
      "loss=52.56393814086914 acc=23\n",
      "loss=52.56723403930664 acc=23\n",
      "loss=52.5613899230957 acc=23\n",
      "loss=52.56560134887695 acc=23\n",
      "loss=52.559104919433594 acc=23\n",
      "loss=52.56416702270508 acc=23\n",
      "loss=52.55706024169922 acc=23\n",
      "loss=52.562889099121094 acc=23\n",
      "loss=52.555259704589844 acc=23\n",
      "loss=52.56174087524414 acc=23\n",
      "loss=52.55372619628906 acc=23\n",
      "loss=52.56071090698242 acc=23\n",
      "loss=52.552398681640625 acc=23\n",
      "loss=52.55976104736328 acc=23\n",
      "loss=52.55126953125 acc=23\n",
      "loss=52.558921813964844 acc=23\n",
      "loss=52.550289154052734 acc=23\n",
      "loss=52.55809020996094 acc=23\n",
      "loss=52.549468994140625 acc=23\n",
      "loss=52.55744552612305 acc=23\n",
      "loss=52.548702239990234 acc=23\n",
      "loss=52.55661392211914 acc=23\n",
      "loss=52.54814529418945 acc=23\n",
      "loss=52.55622100830078 acc=23\n",
      "loss=52.547447204589844 acc=23\n",
      "loss=52.55521774291992 acc=23\n",
      "loss=52.54716110229492 acc=23\n",
      "loss=52.555320739746094 acc=23\n",
      "loss=52.546382904052734 acc=23\n",
      "loss=52.553768157958984 acc=23\n",
      "loss=52.54648208618164 acc=23\n",
      "loss=52.5548210144043 acc=23\n",
      "loss=52.54533767700195 acc=23\n",
      "loss=52.552032470703125 acc=23\n",
      "loss=52.546104431152344 acc=23\n",
      "loss=52.554847717285156 acc=23\n",
      "loss=52.5442008972168 acc=23\n",
      "loss=52.549861907958984 acc=23\n",
      "loss=52.54596710205078 acc=23\n",
      "loss=52.55528259277344 acc=23\n",
      "loss=52.543006896972656 acc=23\n",
      "loss=52.547447204589844 acc=23\n",
      "loss=52.5457878112793 acc=23\n",
      "loss=52.555362701416016 acc=23\n",
      "loss=52.54209899902344 acc=23\n",
      "loss=52.545780181884766 acc=23\n",
      "loss=52.54524612426758 acc=23\n",
      "loss=52.554500579833984 acc=23\n",
      "loss=52.541648864746094 acc=23\n",
      "loss=52.54545211791992 acc=23\n",
      "loss=52.54462432861328 acc=23\n",
      "loss=52.55345153808594 acc=23\n",
      "loss=52.54132843017578 acc=23\n",
      "loss=52.545406341552734 acc=23\n",
      "loss=52.54399871826172 acc=23\n",
      "loss=52.55241775512695 acc=23\n",
      "loss=52.541019439697266 acc=23\n",
      "loss=52.54536819458008 acc=23\n",
      "loss=52.54337692260742 acc=23\n",
      "loss=52.5514030456543 acc=23\n",
      "loss=52.540748596191406 acc=23\n",
      "loss=52.54533767700195 acc=23\n",
      "loss=52.542762756347656 acc=23\n",
      "loss=52.55039978027344 acc=23\n",
      "loss=52.54048538208008 acc=23\n",
      "loss=52.5452995300293 acc=23\n",
      "loss=52.54214096069336 acc=23\n",
      "loss=52.5494270324707 acc=23\n",
      "loss=52.54021453857422 acc=23\n",
      "loss=52.54523849487305 acc=23\n",
      "loss=52.54151916503906 acc=23\n",
      "loss=52.54850387573242 acc=23\n",
      "loss=52.53994369506836 acc=23\n",
      "loss=52.545135498046875 acc=23\n",
      "loss=52.540931701660156 acc=23\n",
      "loss=52.547607421875 acc=23\n",
      "loss=52.53964614868164 acc=23\n",
      "loss=52.54499053955078 acc=23\n",
      "loss=52.540340423583984 acc=23\n",
      "loss=52.54677963256836 acc=23\n",
      "loss=52.539344787597656 acc=23\n",
      "loss=52.5447883605957 acc=23\n",
      "loss=52.5397834777832 acc=23\n",
      "loss=52.545997619628906 acc=23\n",
      "loss=52.53900909423828 acc=23\n",
      "loss=52.54454040527344 acc=23\n",
      "loss=52.53925323486328 acc=23\n",
      "loss=52.5452995300293 acc=23\n",
      "loss=52.53865432739258 acc=23\n",
      "loss=52.54421615600586 acc=23\n",
      "loss=52.538734436035156 acc=23\n",
      "loss=52.54462814331055 acc=23\n",
      "loss=52.53827667236328 acc=23\n",
      "loss=52.543861389160156 acc=23\n",
      "loss=52.53826141357422 acc=23\n",
      "loss=52.544036865234375 acc=23\n",
      "loss=52.537879943847656 acc=23\n",
      "loss=52.54345703125 acc=23\n",
      "loss=52.537784576416016 acc=23\n",
      "loss=52.54347229003906 acc=23\n",
      "loss=52.537471771240234 acc=23\n",
      "loss=52.54303741455078 acc=23\n",
      "loss=52.537330627441406 acc=23\n",
      "loss=52.54295349121094 acc=23\n",
      "loss=52.537071228027344 acc=23\n",
      "loss=52.5425910949707 acc=23\n",
      "loss=52.536888122558594 acc=23\n",
      "loss=52.542449951171875 acc=23\n",
      "loss=52.53663635253906 acc=23\n",
      "loss=52.542144775390625 acc=23\n",
      "loss=52.536441802978516 acc=23\n",
      "loss=52.541954040527344 acc=23\n",
      "loss=52.536224365234375 acc=23\n",
      "loss=52.54169845581055 acc=23\n",
      "loss=52.5360107421875 acc=23\n",
      "loss=52.54148864746094 acc=23\n",
      "loss=52.53579330444336 acc=23\n",
      "loss=52.54123306274414 acc=23\n",
      "loss=52.53557586669922 acc=23\n",
      "loss=52.541015625 acc=23\n",
      "loss=52.535362243652344 acc=23\n",
      "loss=52.540767669677734 acc=23\n",
      "loss=52.535152435302734 acc=23\n",
      "loss=52.54054260253906 acc=23\n",
      "loss=52.534934997558594 acc=23\n",
      "loss=52.540321350097656 acc=23\n",
      "loss=52.53471755981445 acc=23\n",
      "loss=52.54008865356445 acc=23\n",
      "loss=52.53449630737305 acc=23\n",
      "loss=52.53986358642578 acc=23\n",
      "loss=52.5342903137207 acc=23\n",
      "loss=52.539634704589844 acc=23\n",
      "loss=52.53407287597656 acc=23\n",
      "loss=52.539405822753906 acc=23\n",
      "loss=52.53385543823242 acc=23\n",
      "loss=52.539180755615234 acc=23\n",
      "loss=52.53364944458008 acc=23\n",
      "loss=52.538963317871094 acc=23\n",
      "loss=52.5334358215332 acc=23\n",
      "loss=52.53873825073242 acc=23\n",
      "loss=52.53321838378906 acc=23\n",
      "loss=52.538516998291016 acc=23\n",
      "loss=52.53301239013672 acc=23\n",
      "loss=52.538299560546875 acc=23\n",
      "loss=52.532798767089844 acc=23\n",
      "loss=52.53807067871094 acc=23\n",
      "loss=52.532588958740234 acc=23\n",
      "loss=52.5378532409668 acc=23\n",
      "loss=52.532379150390625 acc=23\n",
      "loss=52.537635803222656 acc=23\n",
      "loss=52.532161712646484 acc=23\n",
      "loss=52.537418365478516 acc=23\n",
      "loss=52.531951904296875 acc=23\n",
      "loss=52.537200927734375 acc=23\n",
      "loss=52.531742095947266 acc=23\n",
      "loss=52.5369758605957 acc=23\n",
      "loss=52.531532287597656 acc=23\n",
      "loss=52.53675842285156 acc=23\n",
      "loss=52.53131866455078 acc=23\n",
      "loss=52.536537170410156 acc=23\n",
      "loss=52.531105041503906 acc=23\n",
      "loss=52.536319732666016 acc=23\n",
      "loss=52.53089904785156 acc=23\n",
      "loss=52.536094665527344 acc=23\n",
      "loss=52.530696868896484 acc=23\n",
      "loss=52.535888671875 acc=23\n",
      "loss=52.530479431152344 acc=23\n",
      "loss=52.535667419433594 acc=23\n",
      "loss=52.530269622802734 acc=23\n",
      "loss=52.53545379638672 acc=23\n",
      "loss=52.530059814453125 acc=23\n",
      "loss=52.53522872924805 acc=23\n",
      "loss=52.529842376708984 acc=23\n",
      "loss=52.53501892089844 acc=23\n",
      "loss=52.52964782714844 acc=23\n",
      "loss=52.5348014831543 acc=23\n",
      "loss=52.529422760009766 acc=23\n",
      "loss=52.534584045410156 acc=23\n",
      "loss=52.52922439575195 acc=23\n",
      "loss=52.53437042236328 acc=23\n",
      "loss=52.52901077270508 acc=23\n",
      "loss=52.53416442871094 acc=23\n",
      "loss=52.5287971496582 acc=23\n",
      "loss=52.533939361572266 acc=23\n",
      "loss=52.52859115600586 acc=23\n",
      "loss=52.533721923828125 acc=23\n",
      "loss=52.528385162353516 acc=23\n",
      "loss=52.533512115478516 acc=23\n",
      "loss=52.528160095214844 acc=23\n",
      "loss=52.53329849243164 acc=23\n",
      "loss=52.5279541015625 acc=23\n",
      "loss=52.533077239990234 acc=23\n",
      "loss=52.527748107910156 acc=23\n",
      "loss=52.532867431640625 acc=23\n",
      "loss=52.52753448486328 acc=23\n",
      "loss=52.532649993896484 acc=23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=52.527320861816406 acc=23\n",
      "loss=52.532432556152344 acc=23\n",
      "loss=52.527122497558594 acc=23\n",
      "loss=52.5322151184082 acc=23\n",
      "loss=52.52690505981445 acc=23\n",
      "loss=52.532005310058594 acc=23\n",
      "loss=52.526695251464844 acc=23\n",
      "loss=52.53179168701172 acc=23\n",
      "loss=52.5264892578125 acc=23\n",
      "loss=52.53156661987305 acc=23\n",
      "loss=52.52627182006836 acc=23\n",
      "loss=52.53135681152344 acc=23\n",
      "loss=52.52606964111328 acc=23\n",
      "loss=52.53114318847656 acc=23\n",
      "loss=52.525856018066406 acc=23\n",
      "loss=52.53093338012695 acc=23\n",
      "loss=52.525638580322266 acc=23\n",
      "loss=52.53071212768555 acc=23\n",
      "loss=52.52544021606445 acc=23\n",
      "loss=52.5305061340332 acc=23\n",
      "loss=52.52521896362305 acc=23\n",
      "loss=52.530296325683594 acc=23\n",
      "loss=52.5250129699707 acc=23\n",
      "loss=52.53007888793945 acc=23\n",
      "loss=52.524803161621094 acc=23\n",
      "loss=52.529876708984375 acc=23\n",
      "loss=52.524593353271484 acc=23\n",
      "loss=52.52964782714844 acc=23\n",
      "loss=52.524383544921875 acc=23\n",
      "loss=52.52944564819336 acc=23\n",
      "loss=52.524169921875 acc=23\n",
      "loss=52.529232025146484 acc=23\n",
      "loss=52.52395248413086 acc=23\n",
      "loss=52.529022216796875 acc=23\n",
      "loss=52.523746490478516 acc=23\n",
      "loss=52.528804779052734 acc=23\n",
      "loss=52.52353286743164 acc=23\n",
      "loss=52.528587341308594 acc=23\n",
      "loss=52.52333068847656 acc=23\n",
      "loss=52.52837371826172 acc=23\n",
      "loss=52.52312088012695 acc=23\n",
      "loss=52.528160095214844 acc=23\n",
      "loss=52.522911071777344 acc=23\n",
      "loss=52.527950286865234 acc=23\n",
      "loss=52.522701263427734 acc=23\n",
      "loss=52.52773666381836 acc=23\n",
      "loss=52.522483825683594 acc=23\n",
      "loss=52.527523040771484 acc=23\n",
      "loss=52.522281646728516 acc=23\n",
      "loss=52.52731704711914 acc=23\n",
      "loss=52.522064208984375 acc=23\n",
      "loss=52.527103424072266 acc=23\n",
      "loss=52.521854400634766 acc=23\n",
      "loss=52.526885986328125 acc=23\n",
      "loss=52.52165603637695 acc=23\n",
      "loss=52.526668548583984 acc=23\n",
      "loss=52.52143478393555 acc=23\n",
      "loss=52.52646255493164 acc=23\n",
      "loss=52.5212287902832 acc=23\n",
      "loss=52.5262565612793 acc=23\n",
      "loss=52.52101135253906 acc=23\n",
      "loss=52.526039123535156 acc=23\n",
      "loss=52.52080535888672 acc=23\n",
      "loss=52.52583694458008 acc=23\n",
      "loss=52.520591735839844 acc=23\n",
      "loss=52.525611877441406 acc=23\n",
      "loss=52.5203857421875 acc=23\n",
      "loss=52.52540588378906 acc=23\n",
      "loss=52.520179748535156 acc=23\n",
      "loss=52.52518844604492 acc=23\n",
      "loss=52.51996612548828 acc=23\n",
      "loss=52.524986267089844 acc=23\n",
      "loss=52.519752502441406 acc=23\n",
      "loss=52.524776458740234 acc=23\n",
      "loss=52.51954650878906 acc=23\n",
      "loss=52.524559020996094 acc=23\n",
      "loss=52.519344329833984 acc=23\n",
      "loss=52.52434539794922 acc=23\n",
      "loss=52.519126892089844 acc=23\n",
      "loss=52.524131774902344 acc=23\n",
      "loss=52.5189208984375 acc=23\n",
      "loss=52.5239143371582 acc=23\n",
      "loss=52.51870346069336 acc=23\n",
      "loss=52.523704528808594 acc=23\n",
      "loss=52.518497467041016 acc=23\n",
      "loss=52.523494720458984 acc=23\n",
      "loss=52.51828384399414 acc=23\n",
      "loss=52.523292541503906 acc=23\n",
      "loss=52.518070220947266 acc=23\n",
      "loss=52.523075103759766 acc=23\n",
      "loss=52.51786422729492 acc=23\n",
      "loss=52.522865295410156 acc=23\n",
      "loss=52.51765823364258 acc=23\n",
      "loss=52.52265167236328 acc=23\n",
      "loss=52.517452239990234 acc=23\n",
      "loss=52.522438049316406 acc=23\n",
      "loss=52.51723861694336 acc=23\n",
      "loss=52.5222282409668 acc=23\n",
      "loss=52.517024993896484 acc=23\n",
      "loss=52.52201461791992 acc=23\n",
      "loss=52.516822814941406 acc=23\n",
      "loss=52.52180099487305 acc=23\n",
      "loss=52.516605377197266 acc=23\n",
      "loss=52.5215950012207 acc=23\n",
      "loss=52.51639938354492 acc=23\n",
      "loss=52.521385192871094 acc=23\n",
      "loss=52.51618576049805 acc=23\n",
      "loss=52.521175384521484 acc=23\n",
      "loss=52.5159912109375 acc=23\n",
      "loss=52.520965576171875 acc=23\n",
      "loss=52.51576232910156 acc=23\n",
      "loss=52.520755767822266 acc=23\n",
      "loss=52.51555252075195 acc=23\n",
      "loss=52.52053451538086 acc=23\n",
      "loss=52.515350341796875 acc=23\n",
      "loss=52.520328521728516 acc=23\n",
      "loss=52.51513671875 acc=23\n",
      "loss=52.52011489868164 acc=23\n",
      "loss=52.514930725097656 acc=23\n",
      "loss=52.5199089050293 acc=23\n",
      "loss=52.514713287353516 acc=23\n",
      "loss=52.519691467285156 acc=23\n",
      "loss=52.514503479003906 acc=23\n",
      "loss=52.51948165893555 acc=23\n",
      "loss=52.51429748535156 acc=23\n",
      "loss=52.519264221191406 acc=23\n",
      "loss=52.51409149169922 acc=23\n",
      "loss=52.51905822753906 acc=23\n",
      "loss=52.51386642456055 acc=23\n",
      "loss=52.51884841918945 acc=23\n",
      "loss=52.5136604309082 acc=23\n",
      "loss=52.518638610839844 acc=23\n",
      "loss=52.513458251953125 acc=23\n",
      "loss=52.518428802490234 acc=23\n",
      "loss=52.513240814208984 acc=23\n",
      "loss=52.518218994140625 acc=23\n",
      "loss=52.513031005859375 acc=23\n",
      "loss=52.518009185791016 acc=23\n"
     ]
    }
   ],
   "source": [
    "optim=torch.optim.SGD(model.parameters(),lr=0.010456701)\n",
    "loss=nn.functional.cross_entropy\n",
    "for x in range(500):\n",
    "    for i,j in dl:\n",
    "        pred=model(i)\n",
    "        jj=j.squeeze()\n",
    "        ls=loss(pred,jj)\n",
    "        ls.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        _,pre=torch.max(pred,dim=1)\n",
    "        acc=torch.sum(pre==j)/len(j)\n",
    "        print(f\"loss={ls} acc={acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
